<!DOCTYPE html>
<html lang="en">
    <!-- title -->




<!-- keywords -->




<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="author" content="Orangele">
    <meta name="renderer" content="webkit">
    <meta name="copyright" content="Orangele">
    
    <meta name="keywords" content="hexo,hexo-theme,hexo-blog">
    
    <meta name="description" content>
    <meta http-equiv="Cache-control" content="no-cache">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Some Tips on Tensorflow 2.0 · Orangele&#39;s Blog</title>
    <style type="text/css">
    @font-face {
        font-family: 'Oswald-Regular';
        src: url("/font/Oswald-Regular.ttf");
    }

    body {
        margin: 0;
    }

    header,
    footer,
    .back-top,
    .sidebar,
    .container,
    .site-intro-meta,
    .toc-wrapper {
        display: none;
    }

    .site-intro {
        position: relative;
        z-index: 3;
        width: 100%;
        /* height: 50vh; */
        overflow: hidden;
    }

    .site-intro-placeholder {
        position: absolute;
        z-index: -2;
        top: 0;
        left: 0;
        width: calc(100% + 300px);
        height: 100%;
        background: repeating-linear-gradient(-45deg, #444 0, #444 80px, #333 80px, #333 160px);
        background-position: center center;
        transform: translate3d(-226px, 0, 0);
        animation: gradient-move 2.5s ease-out 0s infinite;
    }

    @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }

</style>

    <link rel="preload" href="/css/style.css?v=20180824" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link rel="stylesheet" href="/css/mobile.css?v=20180824" media="(max-width: 980px)">
    
    <link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    
    <!-- /*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/ -->
<script>
(function( w ){
	"use strict";
	// rel=preload support test
	if( !w.loadCSS ){
		w.loadCSS = function(){};
	}
	// define on the loadCSS obj
	var rp = loadCSS.relpreload = {};
	// rel=preload feature support test
	// runs once and returns a function for compat purposes
	rp.support = (function(){
		var ret;
		try {
			ret = w.document.createElement( "link" ).relList.supports( "preload" );
		} catch (e) {
			ret = false;
		}
		return function(){
			return ret;
		};
	})();

	// if preload isn't supported, get an asynchronous load by using a non-matching media attribute
	// then change that media back to its intended value on load
	rp.bindMediaToggle = function( link ){
		// remember existing media attr for ultimate state, or default to 'all'
		var finalMedia = link.media || "all";

		function enableStylesheet(){
			link.media = finalMedia;
		}

		// bind load handlers to enable media
		if( link.addEventListener ){
			link.addEventListener( "load", enableStylesheet );
		} else if( link.attachEvent ){
			link.attachEvent( "onload", enableStylesheet );
		}

		// Set rel and non-applicable media type to start an async request
		// note: timeout allows this to happen async to let rendering continue in IE
		setTimeout(function(){
			link.rel = "stylesheet";
			link.media = "only x";
		});
		// also enable media after 3 seconds,
		// which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
		setTimeout( enableStylesheet, 3000 );
	};

	// loop through link elements in DOM
	rp.poly = function(){
		// double check this to prevent external calls from running
		if( rp.support() ){
			return;
		}
		var links = w.document.getElementsByTagName( "link" );
		for( var i = 0; i < links.length; i++ ){
			var link = links[ i ];
			// qualify links to those with rel=preload and as=style attrs
			if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
				// prevent rerunning on link
				link.setAttribute( "data-loadcss", true );
				// bind listeners to toggle media back
				rp.bindMediaToggle( link );
			}
		}
	};

	// if unsupported, run the polyfill
	if( !rp.support() ){
		// run once at least
		rp.poly();

		// rerun poly on an interval until onload
		var run = w.setInterval( rp.poly, 500 );
		if( w.addEventListener ){
			w.addEventListener( "load", function(){
				rp.poly();
				w.clearInterval( run );
			} );
		} else if( w.attachEvent ){
			w.attachEvent( "onload", function(){
				rp.poly();
				w.clearInterval( run );
			} );
		}
	}


	// commonjs
	if( typeof exports !== "undefined" ){
		exports.loadCSS = loadCSS;
	}
	else {
		w.loadCSS = loadCSS;
	}
}( typeof global !== "undefined" ? global : this ) );
</script>

    <link rel="icon" href="/assets/favicon.ico">
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js" as="script">
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" as="script">
    <link rel="preload" href="/scripts/main.js" as="script">
    <link rel="preload" as="font" href="/font/Oswald-Regular.ttf" crossorigin>
    <link rel="preload" as="font" href="https://at.alicdn.com/t/font_327081_1dta1rlogw17zaor.woff" crossorigin>
    
    <!-- fancybox -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script>
    <!-- 百度统计  -->
    
    <!-- 谷歌统计  -->
    
</head>

    
        <body class="post-body">
    
    
<header class="header">

    <div class="read-progress"></div>
    <div class="header-sidebar-menu">&#xe775;</div>
    <!-- post页的toggle banner  -->
    
    <div class="banner">
            <div class="blog-title">
                <a href="/" >Orangele&#39;s Blog.</a>
            </div>
            <div class="post-title">
                <a href="#" class="post-name">Some Tips on Tensorflow 2.0</a>
            </div>
    </div>
    
    <a class="home-link" href=/>Orangele's Blog.</a>
</header>
    <div class="wrapper">
        <div class="site-intro" style="







height:50vh;
">
    
    <!-- 主页  -->
    
    
    <!-- 404页  -->
            
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-img" style="background-image: url(https://source.unsplash.com/collection/6787891)"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
            
            Some Tips on Tensorflow 2.0
            <!-- 404 -->
            
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            
            
            <!-- 404 -->
            
        </p>
        <!-- 文章页meta -->
        
            <div class="post-intros">
                <!-- 文章页标签  -->
                
                    <div class= post-intro-tags >
    
        <a class="post-tag" href="javascript:void(0);" data-tags = "Tensorflow">Tensorflow</a>
    
        <a class="post-tag" href="javascript:void(0);" data-tags = "Machine Learning">Machine Learning</a>
    
</div>
                
                
                    <div class="post-intro-read">
                        <span>Word count: <span class="post-count word-count">3.3k</span>Reading time: <span class="post-count reading-time">13 min</span></span>
                    </div>
                
                <div class="post-intro-meta">
                    <span class="post-intro-calander iconfont-archer">&#xe676;</span>
                    <span class="post-intro-time">2019/12/07</span>
                    
                    <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                        <span class="iconfont-archer">&#xe602;</span>
                        <span id="busuanzi_value_page_pv"></span>
                    </span>
                    
                    <span class="shareWrapper">
                        <span class="iconfont-archer shareIcon">&#xe71d;</span>
                        <span class="shareText">Share</span>
                        <ul class="shareList">
                            <li class="iconfont-archer share-qr" data-type="qr">&#xe75b;
                                <div class="share-qrcode"></div>
                            </li>
                            <li class="iconfont-archer" data-type="weibo">&#xe619;</li>
                            <li class="iconfont-archer" data-type="qzone">&#xe62e;</li>
                            <li class="iconfont-archer" data-type="twitter">&#xe634;</li>
                            <li class="iconfont-archer" data-type="facebook">&#xe67a;</li>
                        </ul>
                    </span>
                </div>
            </div>
        
    </div>
</div>
        <script>
 
  // get user agent
  var browser = {
    versions: function () {
      var u = window.navigator.userAgent;
      return {
        userAgent: u,
        trident: u.indexOf('Trident') > -1, //IE内核
        presto: u.indexOf('Presto') > -1, //opera内核
        webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
        gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
        mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
        ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
        android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
        iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
        iPad: u.indexOf('iPad') > -1, //是否为iPad
        webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
        weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
        uc: u.indexOf('UCBrowser') > -1 //是否为android下的UC浏览器
      };
    }()
  }
  console.log("userAgent:" + browser.versions.userAgent);

  // callback
  function fontLoaded() {
    console.log('font loaded');
    if (document.getElementsByClassName('site-intro-meta')) {
      document.getElementsByClassName('intro-title')[0].classList.add('intro-fade-in');
      document.getElementsByClassName('intro-subtitle')[0].classList.add('intro-fade-in');
      var postIntros = document.getElementsByClassName('post-intros')[0]
      if (postIntros) {
        postIntros.classList.add('post-fade-in');
      }
    }
  }

  // UC不支持跨域，所以直接显示
  function asyncCb(){
    if (browser.versions.uc) {
      console.log("UCBrowser");
      fontLoaded();
    } else {
      WebFont.load({
        custom: {
          families: ['Oswald-Regular']
        },
        loading: function () {  //所有字体开始加载
          // console.log('loading');
        },
        active: function () {  //所有字体已渲染
          fontLoaded();
        },
        inactive: function () { //字体预加载失败，无效字体或浏览器不支持加载
          console.log('inactive: timeout');
          fontLoaded();
        },
        timeout: 5000 // Set the timeout to two seconds
      });
    }
  }

  function asyncErr(){
    console.warn('script load from CDN failed, will load local script')
  }

  // load webfont-loader async, and add callback function
  function async(u, cb, err) {
    var d = document, t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (cb) { o.addEventListener('load', function (e) { cb(null, e); }, false); }
    if (err) { o.addEventListener('error', function (e) { err(null, e); }, false); }
    s.parentNode.insertBefore(o, s);
  }

  var asyncLoadWithFallBack = function(arr, success, reject) {
      var currReject = function(){
        reject()
        arr.shift()
        if(arr.length)
          async(arr[0], success, currReject)
        }

      async(arr[0], success, currReject)
  }

  asyncLoadWithFallBack([
    "https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js", 
    "https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js",
    "/lib/webfontloader.min.js"
  ], asyncCb, asyncErr)
</script>        
        <img class="loading" src="/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" />
        <div class="container container-unloaded">
            <main class="main post-page">
    <article class="article-entry">
        <p>最近在学习 tensorflow 2.0 (以下tf2)并尝试改写一些1.x时代的代码。在从1.x转向2.0过程中不可避免地遇到了一些坑，其中大多数来源于建图方式的改变，有些得到了解决有些则还在摸索。2.x版本目前还比较新，在此记录下遇到的问题，希望可以为以后的使用提供参考。在阅读本文前强烈建议阅读 <a href="https://www.tensorflow.org/guide" target="_blank" rel="noopener">tf2 官方指南</a>，尤其是<a href="https://www.tensorflow.org/guide/keras/overview" target="_blank" rel="noopener">Keras</a>部分。<br><a id="more"></a></p>
<h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h2><p>在2.0中最大的改动就是 session 和显式定义 graph 的删除，并同时力推通过 keras 的自定义对象来实现任意模型，因此 keras 在 tf2 中有举足轻重的地位。而伴随其的另一个重大改动是默认开启 <a href="https://www.tensorflow.org/guide/eager" target="_blank" rel="noopener">eager execution</a> 环境, 在该环境下，所有 ops 操作会立即执行并计算出具体的值，而不再像以前那样要先构建一个完整的计算图然后再运行。显然，tf2 强调了代码的灵活性，从而使 tf 具有 define by run 的特点, 来和 Pytorch 竞争研究实验领域的地位。但在工业界，运行速度依然很重要，而图结构大大优化了计算速度，仍然是不可或缺的刚需。那在 tf2 中，该如何实现图呢，目前据我所知有两种方式：1）<a href="https://www.tensorflow.org/guide/function" target="_blank" rel="noopener"><code>tf.function</code></a> 装饰器；2）<a href="https://www.tensorflow.org/api_docs/python/tf/keras/Model" target="_blank" rel="noopener"><code>tf.keras.Model</code></a> 类。具体实现请参考官方指南。可以看出，tf2 意图自然而然地在 eager execution 的基础上添加图的构建，为此加入了AutoGraph的功能以将python语法编写的函数自动转化为图，但是这种隐式的图构建手法对函数和类的设计提出了更高的要求，以便保证默认的 eager execution 部分与图的部分不会产生冲突，对于AutoGraph的各种限制的介绍可以参考<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md" target="_blank" rel="noopener">这里</a>。</p>
<h2 id="2-tf-function装饰器"><a href="#2-tf-function装饰器" class="headerlink" title="2. @tf.function装饰器"></a>2. <code>@tf.function</code>装饰器</h2><p>Tf2 中要求用户将图写作一个函数，原本为定义图中的待输入变量而使用的 <code>tf.placeholder</code> 不复存在，而改为利用函数的参数来输入变量值，并通过函数的返回来获得图的计算结果。我们都知道，图结构一旦建立就是固定且封闭的，以前通过 session 可以运行图并获取任意图中变量的值，但现在我们只能获取函数的返回值，而其余的值被函数给隔离开了。那问题是这种隔离是否能被打破呢，让我们来尝试一下。</p>
<h3 id="2-1-利用Python特性初始化"><a href="#2-1-利用Python特性初始化" class="headerlink" title="2.1. 利用Python特性初始化"></a>2.1. 利用Python特性初始化</h3><p>首先可以想到的是python的函数里的变量并不是完完全全是局部变量，假如某外部变量的同名变量出现在该函数里，同时函数不对其做任何赋值操作，那在调用函数时该变量可以延续其在函数外部的身份，比如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(x)</span>:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>	y = a + x</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>	<span class="keyword">return</span> y</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = <span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>func(<span class="number">1</span>)</span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = <span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>func(<span class="number">1</span>)</span><br><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure></p>
<p>假如将 <code>func</code> 装饰为图，结果会怎么样？<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>@tf.function</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(x)</span>:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>	y = a * x</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>	<span class="keyword">return</span> y</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = <span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>func(<span class="number">1</span>)</span><br><span class="line">&lt;tf.Tensor: id=<span class="number">1265</span>, shape=(), dtype=int32, numpy=<span class="number">1</span>&gt;</span><br></pre></td></tr></table></figure></p>
<p>很好，没报错，结果也是2，这看上去似乎很ok，让我们给a换个值：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = <span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>func(<span class="number">1</span>)</span><br><span class="line">&lt;tf.Tensor: id=<span class="number">1265</span>, shape=(), dtype=int32, numpy=<span class="number">1</span>&gt;</span><br></pre></td></tr></table></figure></p>
<p>不ok了，输出没有变。可见初次调用图函数时外部变量会发挥作用，但图一旦建立，内外变量的联系便切断了。<br>当我们换一个输入：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = <span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>func(<span class="number">2</span>)</span><br><span class="line">&lt;tf.Tensor: id=<span class="number">1271</span>, shape=(), dtype=int32, numpy=<span class="number">4</span>&gt;</span><br></pre></td></tr></table></figure></p>
<p>a又起作用了，那是因为新的图被建立了。<br>tf2 中新建一张图的标准有两个：</p>
<ol>
<li>输入中的数值型变量发生改变 </li>
<li>输入中的张量形状发生改变</li>
</ol>
<p>我们可以验证一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = <span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.convert_to_tensor([<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>func(x)</span><br><span class="line">&lt;tf.Tensor: id=<span class="number">1298</span>, shape=(<span class="number">2</span>,), dtype=int32, numpy=array([<span class="number">1</span>, <span class="number">1</span>])&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = <span class="number">-1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.convert_to_tensor([<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>func(x)</span><br><span class="line">&lt;tf.Tensor: id=<span class="number">1300</span>, shape=(<span class="number">2</span>,), dtype=int32, numpy=array([<span class="number">2</span>, <span class="number">2</span>])&gt;  <span class="comment">#输入改变了，但形状没变，因此没有新建图，</span></span><br><span class="line">                                                                    <span class="comment">#改变的a没有被传入</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = <span class="number">-1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.convert_to_tensor([<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>func(x)</span><br><span class="line">&lt;tf.Tensor: id=<span class="number">1308</span>, shape=(<span class="number">3</span>,), dtype=int32, numpy=array([<span class="number">-2</span>, <span class="number">-2</span>, <span class="number">-2</span>])&gt;  <span class="comment">#输入的形状改变，新建图，a再次发挥作用</span></span><br></pre></td></tr></table></figure></p>
<p>由此我们可以知道，在调用图函数前，可以对函数内的变量进行初始化，而一旦建立了图，这些变量就不应该再在外部被改变，一是因为改变了对图也没有作用，二是在下次新建图时会产生不可预知的影响。</p>
<p>另外，通过声明global变量或者将图函数作为某个类的成员函数，可以做到对全局变量或类成员变量赋值，但这同样也是在新建图时一次性有效的。</p>
<h3 id="2-2-利用tf-summary记录内部变量"><a href="#2-2-利用tf-summary记录内部变量" class="headerlink" title="2.2. 利用tf.summary记录内部变量"></a>2.2. 利用<code>tf.summary</code>记录内部变量</h3><p>1.x中 <code>summary</code> 可以在 session 之外独立获取图内变量值并记录为log，但在2.0我目前还没发现可以这样做的方式。唯一有效的做法是将summary写在图函数中，用法可以参考<a href="https://www.tensorflow.org/api_docs/python/tf/summary" target="_blank" rel="noopener">官方文档</a>，但是我发现官方文档有一处误导：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">writer = tf.summary.create_file_writer(<span class="string">"/tmp/mylogs"</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(step)</span>:</span></span><br><span class="line">	<span class="comment"># other model code would go here</span></span><br><span class="line">	<span class="keyword">with</span> writer.as_default():</span><br><span class="line">		tf.summary.scalar(<span class="string">"my_metric"</span>, <span class="number">0.5</span>, step=step)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">	my_func(step) <span class="comment"># Wrong!</span></span><br><span class="line">	writer.flush()</span><br></pre></td></tr></table></figure></p>
<p>就如我们刚才所验证的，图函数一旦传入不同的数值型参数就会新建图，在这里显然<code>my_func(step)</code>在不断地新建图，这样会使其远远慢于直接eagerly地使用函数。正确的做法应该是把 <code>step</code> 转化为标量张量：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">	my_func(tf.convert_to_tensor(step,tf.int64))</span><br><span class="line">	writer.flush()</span><br></pre></td></tr></table></figure></p>
<p>注意一定要指定类型为 <code>tf.int64</code>，因为默认转化类型 <code>tf.int32</code> 会使 <code>SummaryWriter</code> 报错。</p>
<h2 id="3-Keras-类"><a href="#3-Keras-类" class="headerlink" title="3. Keras 类"></a>3. Keras 类</h2><p>现在一个最大的感受就是 keras 被糅进了 tf 的日常使用当中。当 keras 拥有了可以高度自定义的类，与其说它是 tf 的高层组件，不如说它提供了一个面向对象的建模手段。总得来说，keras提供了层(layer)级的抽象和模型(model)级的抽象，使用者可以将内建的层或者自定义层在模型中任意组合。</p>
<p>事实上，编写自定义层的过程和编写图函数的过程并没有太大区别，而假如对训练过程没有什么特殊需求，完全可以通过 <code>compile</code> 和 <code>fit</code> 轻松实现训练而不必自己写训练过程， keras 的模型在 compile 时是默认建图的，同时 keras 的模型还支持轻松地保存/载入参数和保存/载入模型。但是图函数风格的模型会更具可控性、更加易于调试。在图函数里你可以直观地编写代码而不必关心keras类那些层层包裹的复杂接口。自建图函数 or 使用 keras 类？这个问题官方在 <a href="https://www.tensorflow.org/guide/keras/functional#when_to_use_the_functional_api" target="_blank" rel="noopener">when to use the functional api</a> 有过简短的讨论。</p>
<p>我个人参考教程对 keras 类的自定义做了些探索，其自定义要素体现在这几方面：</p>
<ul>
<li>model派生类的自定义</li>
<li>layer派生类的自定义</li>
<li>损失函数的自定义</li>
<li>训练过程的设置</li>
</ul>
<p>越复杂越深度的自定义越容易遇到不可预知的问题，这就要求对这些类有深入的了解。关于自定义layer和model的基础可以参考<a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models" target="_blank" rel="noopener">这部分教程</a>，自定义损失函数则在<a href="https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_losses" target="_blank" rel="noopener">这部分</a>有叙述。</p>
<h3 id="3-3-自定义复杂损失函数"><a href="#3-3-自定义复杂损失函数" class="headerlink" title="3.3. 自定义复杂损失函数"></a>3.3. 自定义复杂损失函数</h3><p>从上述教程中可知，自定义损失函数有两种方法：</p>
<ol>
<li>定义一个以真实值和预测值为参数的函数 <code>func(true, pred)</code></li>
<li>定义一个 tf.keras.losses.Loss 的派生类，可以在实例化时传递真实值和预测值以外的参数</li>
</ol>
<p>在用第一种方法时，我遇到了一个问题，那就是当我在 <code>model.fit()</code> 中设置 <code>sample_weight</code> 时，会报如下错误</p>
<blockquote>
<p>InvalidArgumentError: Can not squeeze dim[0], expected a dimension of 1, got 1024</p>
</blockquote>
<p>其中1024是我的 batch size。其实原本我就很疑惑，假如根据教程我的损失函数返回的是一个标量，那这个 <code>sample_weight</code> 是如何神奇地在样本粒度起作用的。我曾尝试使损失函数接收 <code>sample_weight</code> 变为 <code>func(y_true, y_pred, sample_weight)</code> ，依然报错。结果当我令原本的损失函数输出一个 <code>shape</code> 为 <code>(batch_size,)</code> 的张量时，一切就正常了。我们可以合理推测，<code>sample_weight</code> 并不会传递给损失函数，而是和损失函数得到的结果相乘然后取和作为最终的损失。而当查看 <a href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/Loss#__call__" target="_blank" rel="noopener"><code>tf.keras.losses.Loss</code> 的文档</a>时发现，它的 <code>__call__()</code> 函数有三个参数 <code>y_true</code>, <code>y_pred</code> 和 <code>sample_weight</code>， 而 <code>call()</code> 则只有 <code>y_true</code>, <code>y_pred</code>，因此可以推测我们定义的损失函数可能是被当作一个 <code>tf.keras.losses.Loss</code> 实例的 <code>call()</code> 函数来使用，在调用该实例时，<code>__call__()</code> 会接收 <code>sample_weight</code> 并调用 <code>call()</code>，将 <code>sample_weight</code> 与 <code>call()</code> 返回的结果相乘并求和然后返回最终结果。</p>
<p>可能有空时我会去看看源码来验证具体的机制。目前的结论就是：当设置有 <code>sample_weight</code> 时，损失函数应该输出一个 <code>shape</code> 为 <code>(batch_size,)</code> 的张量。使用第二种方法时道理也是一样的，只是损失函数变成了 <code>call()</code> 函数。</p>
<p>假如想在训练过程中，获取内部变量作为损失的部分，可以在自定义 layer 时调用 <code>self.add_loss(loss_value)</code>。假如是用 model 的 <code>fit()</code> 函数来训练，则添加的损失会被自动加进最终损失中。假如是自己写的训练过程，则可以用 model.losses 来获取添加的损失，注意 model.losses 是一个列表，单次计算中每次调用 <code>add_loss(loss_value)</code> 都会把 <code>loss_value</code> 添加到列表中，每次计算列表都会被清空。</p>
<h3 id="3-2-trainable-weights-的消失"><a href="#3-2-trainable-weights-的消失" class="headerlink" title="3.2. trainable_weights 的消失"></a>3.2. trainable_weights 的消失</h3><p>先来看一个自定义 layer 的例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">layer</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_features, order)</span>:</span></span><br><span class="line">        super(layer, self).__init__()</span><br><span class="line">        self.w = [<span class="literal">None</span>] * n_features</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, order + <span class="number">1</span>):</span><br><span class="line">            rnd_weights = tf.random.uniform((n_features,<span class="number">1</span>), minval = <span class="number">-0.01</span>, maxval = <span class="number">0.01</span>)</span><br><span class="line">            self.w[i - <span class="number">1</span>] = tf.Variable(<span class="number">0.1</span>, trainable=<span class="literal">True</span>, name=<span class="string">'coefficient_'</span> + str(i))</span><br><span class="line">        self.b = tf.Variable(<span class="number">0.1</span>, trainable=<span class="literal">True</span>, name=<span class="string">'bias'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        outputs = <span class="number">0</span> + self.b</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, order+<span class="number">1</span>):</span><br><span class="line">            outputs += tf.matmul(tf.pow(inputs, i),self.w[i<span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure></p>
<p>在这里我根据输入 <code>order</code> 动态构建了了 <code>w</code> 列表内的可训练参数。可是当我最终通过 <code>layer.weights</code> 或者 <code>layer.trainabel_weights</code> 查看参数时却只有 <code>bias</code>, 也就是 <code>b</code>。可见文中 for 循环定义可训练参数会影响模型拾取参数的过程。这种情况到底应该怎么处理最好我也拿不准，希望有高人指点。我的办法是改用 <code>add_weight</code> 接口：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">layer</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_features, order)</span>:</span></span><br><span class="line">        super(layer, self).__init__()</span><br><span class="line">        self.w = [<span class="literal">None</span>] * n_features</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, order + <span class="number">1</span>):</span><br><span class="line">            rnd_weights = tf.random_uniform_initializer(minval = <span class="number">-0.01</span>, maxval = <span class="number">0.01</span>)</span><br><span class="line">            self.w[i - <span class="number">1</span>] = self.add_weight(initializer=rnd_weights, trainable=<span class="literal">True</span>, name=<span class="string">'coefficient_'</span> + str(i), shape=(n_features,<span class="number">1</span>))</span><br><span class="line">        self.b = tf.Variable(<span class="number">0.1</span>, trainable=<span class="literal">True</span>, name=<span class="string">'bias'</span>)</span><br></pre></td></tr></table></figure></p>
<p>注意 <code>add_weight</code> 的 initializer 只接受不带参的 callable，假如想用固定值，<code>lambda:value</code> 是一个简单的方法。</p>
<h3 id="3-4-利用tf-data-Dataset"><a href="#3-4-利用tf-data-Dataset" class="headerlink" title="3.4. 利用tf.data.Dataset"></a>3.4. 利用tf.data.Dataset</h3><p>关于 <code>tf.data.Dataset</code> 的详细教程可以在<a href="https://www.tensorflow.org/guide/data" target="_blank" rel="noopener">这里</a>看到。Dataset 旨在用来接受和转换不同类型的数据源，并提供一系列简单的数据预处理处理手段，并最终作为 <code>model.fit()</code> 的输入。但值得注意的是它和 <code>model.fit()</code> 的参数存在两个冲突：</p>
<ul>
<li>batch_size</li>
<li>sample_weight</li>
</ul>
<p>也就是说一旦使用了 dataset ， 就不能在 fit 时设定这两个参数，而必须得在 dataset 上动手。<br>这两个功能在 dataset 上的实现分别为：</p>
<ul>
<li><code>train_datase = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)</code></li>
<li>将sample_weight作为新建 dataset 时传入的 tuple 的第三个元素，即：<br><code>train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train, sample_weight))</code><br>(可参考<a href="https://www.tensorflow.org/guide/keras/train_and_evaluate#using_sample_weighting_and_class_weighting" target="_blank" rel="noopener">这一段</a>)</li>
</ul>
<h2 id="4-待解决问题"><a href="#4-待解决问题" class="headerlink" title="4.待解决问题"></a>4.待解决问题</h2><h3 id="4-1-图函数对SparseTensor的兼容问题"><a href="#4-1-图函数对SparseTensor的兼容问题" class="headerlink" title="4.1. 图函数对SparseTensor的兼容问题"></a>4.1. 图函数对SparseTensor的兼容问题</h3><p>在使用过程中我发现图函数对SparseTensor并不完全兼容，而且处理SparseTensor的速度上和1代相比有很大差距，这可能是 autograph 还尚不完全成熟，因为对于SparseTensor来说即使是同样的 shape，其 index 和 value 的大小也会不同。考虑过将 SparseTensor 转化为 dataset，但 dataset 的训练必须用 <code>model.fit()</code> ，过程中会出现如下问题。</p>
<h3 id="4-2-使用-model-fit-训练时的权重消失问题"><a href="#4-2-使用-model-fit-训练时的权重消失问题" class="headerlink" title="4.2. 使用 model.fit() 训练时的权重消失问题"></a>4.2. 使用 <code>model.fit()</code> 训练时的权重消失问题</h3><p>3.2里虽然通过 <code>add_weight()</code> 使权重得以自动加入模型的 <code>trainable_weights</code> ，但是用 <code>model.fit()</code> 训练模型时依然没法训练那些权重 ，因此不得不自写训练 loop ，而4.1的问题只能靠特定情况下转为 eager execution 来解决。</p>

    </article>
    <!-- license  -->
    
        <div class="license-wrapper">
            <p>Author：<a href="https://orangelle.github.io">Orangele</a>
            <p>原文链接：<a href="https://orangelle.github.io/2019/12/07/Some-tips-on-tensorflow-2-0/">https://orangelle.github.io/2019/12/07/Some-tips-on-tensorflow-2-0/</a>
            <p>发表日期：<a href="https://orangelle.github.io/2019/12/07/Some-tips-on-tensorflow-2-0/">December 7th 2019, 2:00:04 am</a>
            <p>更新日期：<a href="https://orangelle.github.io/2019/12/07/Some-tips-on-tensorflow-2-0/">December 7th 2019, 2:13:57 am</a>
            <p>版权声明：本文采用<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可</p>
        </div>
    
    <!-- paginator  -->
    <ul class="post-paginator">
        <li class="next">
            
                <div class="nextSlogan">Next Post</div>
                <a href= "/2019/12/07/Differences-of-dot-matmul-multiply-outer-in-numpy/" title= "Differences of dot(), matmul(), multiply(), outer(), *, @ in numpy">
                    <div class="nextTitle">Differences of dot(), matmul(), multiply(), outer(), *, @ in numpy</div>
                </a>
            
        </li>
        <li class="previous">
            
                <div class="prevSlogan">Previous Post</div>
                <a href= "/2019/09/07/MLE,MAP,BE/" title= "Maximum Likelihood Estimation, Maximum A Posterior Estimation and Bayesian Estimation">
                    <div class="prevTitle">Maximum Likelihood Estimation, Maximum A Posterior Estimation and Bayesian Estimation</div>
                </a>
            
        </li>
    </ul>
    <!-- 评论插件 -->
    <!-- 来必力City版安装代码 -->

<!-- City版安装代码已完成 -->
    
    
    <!-- partial('_partial/comment/changyan') -->
    <!--PC版-->


    
    

    <!-- 评论 -->
</main>
            <!-- profile -->
            
        </div>
        <footer class="footer footer-unloaded">
    <!-- social  -->
    
    <div class="social">
        
    
        
            
                <a href="mailto:raku.lxm@gmail.com" class="iconfont-archer email" title=email ></a>
            
        
    
        
            
                <a href="https://github.com/orangelle" class="iconfont-archer github" target="_blank" title=github></a>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    </div>
    
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank">Archer</a></span>
    </div>
    <!-- 不蒜子  -->
    
    <div class="busuanzi-container">
    
     
    <span id="busuanzi_container_site_pv">PV: <span id="busuanzi_value_site_pv"></span> :)</span>
    
    </div>
    
</footer>
    </div>
    <!-- toc -->
    
    <div class="toc-wrapper" style=
    







top:50vh;

    >
        <div class="toc-catalog">
            <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
        </div>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-概述"><span class="toc-number">1.</span> <span class="toc-text">1. 概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-tf-function装饰器"><span class="toc-number">2.</span> <span class="toc-text">2. @tf.function装饰器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-利用Python特性初始化"><span class="toc-number">2.1.</span> <span class="toc-text">2.1. 利用Python特性初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-利用tf-summary记录内部变量"><span class="toc-number">2.2.</span> <span class="toc-text">2.2. 利用tf.summary记录内部变量</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Keras-类"><span class="toc-number">3.</span> <span class="toc-text">3. Keras 类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-自定义复杂损失函数"><span class="toc-number">3.1.</span> <span class="toc-text">3.3. 自定义复杂损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-trainable-weights-的消失"><span class="toc-number">3.2.</span> <span class="toc-text">3.2. trainable_weights 的消失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-利用tf-data-Dataset"><span class="toc-number">3.3.</span> <span class="toc-text">3.4. 利用tf.data.Dataset</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-待解决问题"><span class="toc-number">4.</span> <span class="toc-text">4.待解决问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-图函数对SparseTensor的兼容问题"><span class="toc-number">4.1.</span> <span class="toc-text">4.1. 图函数对SparseTensor的兼容问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-使用-model-fit-训练时的权重消失问题"><span class="toc-number">4.2.</span> <span class="toc-text">4.2. 使用 model.fit() 训练时的权重消失问题</span></a></li></ol></li></ol>
    </div>
    
    <div class="back-top iconfont-archer">&#xe639;</div>
    <div class="sidebar sidebar-hide">
    <ul class="sidebar-tabs sidebar-tabs-active-0">
        <li class="sidebar-tab-archives"><span class="iconfont-archer">&#xe67d;</span><span class="tab-name">Archive</span></li>
        <li class="sidebar-tab-tags"><span class="iconfont-archer">&#xe61b;</span><span class="tab-name">Tag</span></li>
        <li class="sidebar-tab-categories"><span class="iconfont-archer">&#xe666;</span><span class="tab-name">Cate</span></li>
    </ul>
    <div class="sidebar-content sidebar-content-show-archive">
          <div class="sidebar-panel-archives">
    <!-- 在ejs中将archive按照时间排序 -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="total-and-search">
        <div class="total-archive">
        Total : 4
        </div>
        <!-- search  -->
        
    </div>
    
    <div class="post-archive">
    
    
    
    
    <div class="archive-year"> 2019 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/07</span><a class="archive-post-title" href= "/2019/12/07/Some-tips-on-tensorflow-2-0/" >Some Tips on Tensorflow 2.0</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/07</span><a class="archive-post-title" href= "/2019/12/07/Differences-of-dot-matmul-multiply-outer-in-numpy/" >Differences of dot(), matmul(), multiply(), outer(), *, @ in numpy</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/07</span><a class="archive-post-title" href= "/2019/09/07/MLE,MAP,BE/" >Maximum Likelihood Estimation, Maximum A Posterior Estimation and Bayesian Estimation</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/09</span><a class="archive-post-title" href= "/2019/07/09/Pratical_IO_for_C++/" >Practical IO for C++</a>
        </li>
    
    </div>
  </div>
        <div class="sidebar-panel-tags">
    <div class="sidebar-tags-name">
    
        <span class="sidebar-tag-name" data-tags="Numpy"><span class="iconfont-archer">&#xe606;</span>Numpy</span>
    
        <span class="sidebar-tag-name" data-tags="C++"><span class="iconfont-archer">&#xe606;</span>C++</span>
    
        <span class="sidebar-tag-name" data-tags="Tensorflow"><span class="iconfont-archer">&#xe606;</span>Tensorflow</span>
    
        <span class="sidebar-tag-name" data-tags="Machine Learning"><span class="iconfont-archer">&#xe606;</span>Machine Learning</span>
    
        <span class="sidebar-tag-name" data-tags="Statistics"><span class="iconfont-archer">&#xe606;</span>Statistics</span>
    
    </div>
    <div class="iconfont-archer sidebar-tags-empty">&#xe678;</div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
    缺失模块。<br/>
    1、请确保node版本大于6.2<br/>
    2、在博客根目录（注意不是archer根目录）执行以下命令：<br/>
    <span style="color: #f75357; font-size: 1rem; line-height: 2rem;">npm i hexo-generator-json-content --save</span><br/>
    3、在根目录_config.yml里添加配置：
    <pre style="color: #787878; font-size: 0.6rem;">
jsonContent:
  meta: false
  pages: false
  posts:
    title: true
    date: true
    path: true
    text: false
    raw: false
    content: false
    slug: false
    updated: false
    comments: false
    link: false
    permalink: false
    excerpt: false
    categories: true
    tags: true</pre>
    </div> 
    <div class="sidebar-tags-list"></div>
</div>
        <div class="sidebar-panel-categories">
    <div class="sidebar-categories-name">
    
        <span class="sidebar-category-name" data-categories="Coding"><span class="iconfont-archer">&#xe60a;</span>Coding</span>
    
        <span class="sidebar-category-name" data-categories="Theory"><span class="iconfont-archer">&#xe60a;</span>Theory</span>
    
    </div>
    <div class="iconfont-archer sidebar-categories-empty">&#xe678;</div>
    <div class="sidebar-categories-list"></div>
</div>
    </div>
</div> 
    <script>
    var siteMeta = {
        root: "/",
        author: "Orangele"
    }
</script>
    <!-- CDN failover -->
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
    <script type="text/javascript">
        if (typeof window.$ === 'undefined')
        {
            console.warn('jquery load from jsdelivr failed, will load local script')
            document.write('<script src="/lib/jquery.min.js">\x3C/script>')
        }
    </script>
    <script src="/scripts/main.js"></script>
    <!-- algolia -->
    
    <!-- busuanzi  -->
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
    <!-- CNZZ  -->
    
    </div>
    <!-- async load share.js -->
    
        <script src="/scripts/share.js" async></script>    
     
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>
</html>


